# MiniGPT Model Training with FineWeb Dataset

This repository contains code for training a mini-GPT model with approximately 124 million parameters on the **FineWeb Dataset** (10 billion tokens). The trained model is validated on the **HellaSwag** dataset to assess its commonsense reasoning abilities.

## Overview

- **Training Dataset**: FineWeb (10 billion tokens)
- **Model**: Mini-GPT with 124M parameters
- **Validation Dataset**: HellaSwag

